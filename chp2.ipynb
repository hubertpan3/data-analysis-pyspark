{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#In this chapter we will be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/07/21 14:03:55 WARN Utils: Your hostname, LAPTOP-CDHH1LA0 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/07/21 14:03:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/21 14:03:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/21 14:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create or fetch a new spark session with a specified job name\n",
    "spark = SparkSession.builder.appName(\"Analyzing Stuff\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing Stuff</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Analyzing Stuff>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the spark context\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark can  operate either on Resilient Distributed Data sets (RDDs) or DataFrames (DFs).\n",
    "\n",
    "An RDD is basically a collection of objects that can be treated as rows in a database table.\n",
    "We typically operate on the objects on a row by row basis\n",
    "\n",
    "A DF is basically a collection of data series that can be treated as columns in a database table. Heavily influenced by PANDAS\n",
    "We typically operate on the data on a column by column basis.\n",
    "The `pyspark.sql` package allows for us to use SQL to operate on dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7f63e53709d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use spark.read to pull data into a data frame\n",
    "spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above `spark.read` can from several data formats such as csv, json, jdbc, orc, and parquet.\n",
    "orc and parquet are competing data formats designed for large amounts of columnar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next we are going to create a dataframe from a text file containing the contents of pride and prejudice\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "# the resulting dataframe has a column called \"value\" with type \"string\"\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen'),\n",
       " Row(value=''),\n",
       " Row(value='This eBook is for the use of anyone anywhere at no cost and with'),\n",
       " Row(value='almost no restrictions whatsoever.  You may copy it, give it away or'),\n",
       " Row(value='re-use it under the terms of the Project Gutenberg License included'),\n",
       " Row(value='with this eBook or online at www.gutenberg.org'),\n",
       " Row(value=''),\n",
       " Row(value=''),\n",
       " Row(value='Title: Pride and Prejudice'),\n",
       " Row(value='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"heading\" the first 10 values seems to indicate that each line in the text file became a row in the value column\n",
    "book.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can view a more detailed schema in two ways\n",
    "book.printSchema() # which generates a tree based view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('value', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# and \n",
    "print(book.dtypes) # which would allow for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'active',\n",
       " 'addArtifact',\n",
       " 'addArtifacts',\n",
       " 'addTag',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'clearTags',\n",
       " 'client',\n",
       " 'conf',\n",
       " 'copyFromLocalToFs',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'getTags',\n",
       " 'interruptAll',\n",
       " 'interruptOperation',\n",
       " 'interruptTag',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'removeTag',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'udtf',\n",
       " 'version']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when learning and exploring python it is often helpful to list python functions and attributes with dir\n",
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "\n",
      "    A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "    tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "    To create a :class:`SparkSession`, use the following builder pattern:\n",
      "\n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "\n",
      "    .. autoattribute:: builder\n",
      "       :annotation:\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Create a Spark session.\n",
      "\n",
      "    >>> spark = (\n",
      "    ...     SparkSession.builder\n",
      "    ...         .master(\"local\")\n",
      "    ...         .appName(\"Word Count\")\n",
      "    ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "    ...         .getOrCreate()\n",
      "    ... )\n",
      "\n",
      "    Create a Spark session with Spark Connect.\n",
      "\n",
      "    >>> spark = (\n",
      "    ...     SparkSession.builder\n",
      "    ...         .remote(\"sc://localhost\")\n",
      "    ...         .appName(\"Word Count\")\n",
      "    ...         .config(\"spark.some.config.option\", \"some-value\")\n",
      "    ...         .getOrCreate()\n",
      "    ... )  # doctest: +SKIP\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# or pull doc strings with \n",
    "print(spark.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|                                                               value|\n",
      "+--------------------------------------------------------------------+\n",
      "|  The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen|\n",
      "|                                                                    |\n",
      "|    This eBook is for the use of anyone anywhere at no cost and with|\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or|\n",
      "| re-use it under the terms of the Project Gutenberg License included|\n",
      "|                      with this eBook or online at www.gutenberg.org|\n",
      "|                                                                    |\n",
      "|                                                                    |\n",
      "|                                          Title: Pride and Prejudice|\n",
      "|                                                                    |\n",
      "|                                                 Author: Jane Austen|\n",
      "|                                                                    |\n",
      "|                         Posting Date: August 26, 2008 [EBook #1342]|\n",
      "|                                            Release Date: June, 1998|\n",
      "|                                        Last Updated: March 10, 2018|\n",
      "|                                                                    |\n",
      "|                                                   Language: English|\n",
      "|                                                                    |\n",
      "|                                       Character set encoding: UTF-8|\n",
      "|                                                                    |\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in addition to using \"head\" to display df contents, we can also use show\n",
    "book.show(n=20, truncate=100, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next we can transform our raw data for use in processing\n",
    "\n",
    "from pyspark.sql.functions import split \n",
    "\n",
    "lines = book.select( # selects data\n",
    "    split(book.value, \" \") # splits the contents of each element in the specified column (series) by the specified token\n",
    "        .alias(\"line\") # renames the transformed column to a new name\n",
    "    )\n",
    "\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, :class:`Column`, or list\n",
      "            column names (string) or expressions (:class:`Column`).\n",
      "            If one of the column names is '*', that column is expanded to include all columns\n",
      "            in the current :class:`DataFrame`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "            A DataFrame with subset (or all) of columns.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "\n",
      "        Select all columns in the DataFrame.\n",
      "\n",
      "        >>> df.select('*').show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "\n",
      "        Select a column with other expressions in the DataFrame.\n",
      "\n",
      "        >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
      "        +-----+---+\n",
      "        | name|age|\n",
      "        +-----+---+\n",
      "        |Alice| 12|\n",
      "        |  Bob| 15|\n",
      "        +-----+---+\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as seen in the docs, select works by projecting 1 or more columns from a dataframe into a new one\n",
    "print(book.select.__doc__)\n",
    "\n",
    "# these columns can either be specified as strings or column objects\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "book.select(book.value) # select the value column from the book df\n",
    "book.select(book[\"value\"]) # ditto\n",
    "book.select(col(\"value\")) # ditto\n",
    "book.select(\"value\") # ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines2: DataFrame[split(value,  , -1): array<string>]\n",
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+--------------------+\n",
      "| split(value,  , -1)|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting without aliasing\n",
    "# notice that without aliasing the column is named after the transformation used to generate the column\n",
    "\n",
    "from pyspark.sql.functions import col, split \n",
    "\n",
    "lines2 = book.select(split(col(\"value\"), \" \", limit = -1)) # specifies the transformation to convert each sentence to an array of words\n",
    "print(f\"lines2: {lines2}\") # print container type\n",
    "\n",
    "lines2.printSchema() # print schema\n",
    "\n",
    "lines2.show(5) # trigger \"action\" which causes the computation to happen so we can display the results for the first 5 elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# columns can be renamed either via \"alias\" or \"withColumnRenamed\"\n",
    "\n",
    "lines3 = book.select(split(book.value, \" \"))\n",
    "lines3.printSchema()\n",
    "# vs with alias which operates on the column\n",
    "book.select(split(book.value, \" \").alias(\"line\")).printSchema()\n",
    "# vs with withColumnRenamed which operates on the dataframe and requires that you specify the target column and new column name\n",
    "lines3.withColumnRenamed(\"split(value,  , -1)\", \"line\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lower case the words\n",
    "from pyspark.sql.functions import lower \n",
    "words_lower = words.select(lower(words.word).alias(\"word_lower\"))\n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove non words\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(words_lower.word_lower, \"[a-z]+\", 0).alias(\"word\")\n",
    ")\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|            numbers|\n",
      "+-------------------+\n",
      "|    [1, 2, 3, 4, 5]|\n",
      "|[5, 6, 7, 8, 9, 10]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exo_2_1_df = spark.createDataFrame([{\"numbers\": [1,2,3,4,5]}, {\"numbers\": [5,6,7,8,9,10]}])\n",
    "exo_2_1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution_2_1_df contains 11 records\n",
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solution_2_1_df = exo_2_1_df.select(explode(exo_2_1_df.numbers))\n",
    "print(f\"solution_2_1_df contains {solution_2_1_df.count()} records\")\n",
    "solution_2_1_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering rows\n",
    "words_nonnull = words_clean.filter(words_clean.word != \"\")\n",
    "words_nonnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonnull_by_negate = words_clean.filter(~(words_clean.word == \"\"))\n",
    "words_nonnull_by_negate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'word[starts]'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(words_nonnull.word)\n",
    "words_nonnull.word.starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      "\n",
      "[('col1', 'string'), ('col2', 'string'), ('col3', 'bigint')]\n",
      "string\n",
      "string\n",
      "bigint\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# you can create dataframes by specifying an array of row arrays followed by an array of column names\n",
    "exo2_2_df = spark.createDataFrame(\n",
    "    [[\"test\", \"more test\", 100], [\"bob\", \"joe\", 20]], # array of row arrays\n",
    "    [\"col1\", \"col2\", \"col3\"] # column array\n",
    ")\n",
    "\n",
    "exo2_2_df.printSchema()\n",
    "\n",
    "# two automatically count the number of non string columns we can iterate of the dtypes list\n",
    "print(exo2_2_df.dtypes)\n",
    "\n",
    "non_string_count = 0\n",
    "for type in exo2_2_df.dtypes:\n",
    "    print(type[1])\n",
    "    if 'string' != type[1]:\n",
    "        non_string_count = non_string_count + 1\n",
    "print(non_string_count)\n",
    "\n",
    "# or in the book solution, we can use a list comprehension \n",
    "print(len([x for x, y, in exo2_2_df.dtypes if y!= \"string\"]))\n",
    "\n",
    "testB = [(\"1\", 2, 3), (\"1\", 3, 3)]\n",
    "print(len([x for x, y, z, in testB if z == 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number_of_char: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- number_of_char: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exercise 2.3, alias vs withColumnRenamed\n",
    "\n",
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "exo2_3_df =  (\n",
    "    spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "    .select(length(col(\"value\")))\n",
    "    .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ")\n",
    "\n",
    "exo2_3_df.printSchema()\n",
    "\n",
    "exo2_3_df_alias = (\n",
    "    spark.read.text(\"data/gutenberg_books/1342-0.txt\")\n",
    "    .select(length(col(\"value\")).alias(\"number_of_char\"))\n",
    ")\n",
    "\n",
    "exo2_3_df_alias.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: long (nullable = true)\n",
      "\n",
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|key|100000| 20000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exercise 2.4\n",
    "from pyspark.sql.functions import col, greatest\n",
    "\n",
    "exo2_4_df = spark.createDataFrame(\n",
    "    [[\"key\", 10_0000, 20_000]], \n",
    "    [\"key\", \"value1\", \"value2\"]\n",
    ")\n",
    "\n",
    "exo2_4_df.printSchema()\n",
    "exo2_4_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|key|maximum_value|\n",
      "+---+-------------+\n",
      "|key|       100000|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2.4\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "        col(\"key\"),\n",
    "        greatest(col(\"value1\"), col(\"value2\"))\n",
    "        .alias(\"maximum_value\")\n",
    "    )\n",
    "    exo2_4_mod.show()\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122157\n",
      "66899\n"
     ]
    }
   ],
   "source": [
    "# q2.5\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(words.word).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(words_lower.word_lower, \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "words_nonnull = words_clean.where(col(\"word\") != \"\")\n",
    "print(words_nonnull.count())\n",
    "words_no_is = words_nonnull.where(words_nonnull.word != \"is\")\n",
    "words_no_is.count()\n",
    "\n",
    "words_longer_than_three = words_clean.where(length(words_clean.word) > 3)\n",
    "print(words_longer_than_three.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7088\n"
     ]
    }
   ],
   "source": [
    "# exercise 2.6 -> removing stop words\n",
    "words_no_stop = words_nonnull.where(words_nonnull.word.isin([\"is\", \"not\", \"the\", \"if\"]))\n",
    "print(words_no_stop.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exercise 2.7 fix bug\n",
    "from pyspark.sql.functions import col, split\n",
    " \n",
    "try:\n",
    "    book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "    #book = book.printSchema() # this would have unassigned the dataframe\n",
    "    book.printSchema()\n",
    "    lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [word: string], value: [word: string], type: GroupBy]\n",
      "DataFrame[word: string, count: bigint]\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1750|\n",
      "| she| 1668|\n",
      "|that| 1487|\n",
      "|  it| 1482|\n",
      "| not| 1427|\n",
      "| you| 1301|\n",
      "|  he| 1296|\n",
      "|  be| 1257|\n",
      "| his| 1247|\n",
      "|  as| 1174|\n",
      "| had| 1170|\n",
      "|with| 1092|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouping \n",
    "groups = words_nonnull.groupby(words_nonnull.word)\n",
    "print(groups)\n",
    "results = groups.count()\n",
    "print(results)\n",
    "results.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     3|28154|\n",
      "|     2|23349|\n",
      "|     4|21766|\n",
      "|     5|11880|\n",
      "|     6| 9218|\n",
      "|     7| 8620|\n",
      "|     9| 5141|\n",
      "|     8| 5101|\n",
      "|     1| 3755|\n",
      "|    10| 2446|\n",
      "|    11| 1378|\n",
      "|    12|  811|\n",
      "|    13|  391|\n",
      "|    14|  107|\n",
      "|    15|   32|\n",
      "|    16|    5|\n",
      "|    17|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = words_nonnull.select(length(words_nonnull.word).alias(\"length\")).groupby(\"length\")\n",
    "b = a.count()\n",
    "b.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a \n",
    "b.orderBy(col(\"count\").desc()).write.csv(\"./data/simple_count_csv\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1750|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = words_nonnull.groupBy(col(\"word\")).count()\n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "results.coalesce(1).write.csv(\"./data/simple_word_count_csv\", \"overwrite\") # coalesce shrinks the partitions down to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F # allows you to pull all the functions like col, explode, lower, et. al.\n",
    "\n",
    "# you can also make the program much more compact but not assigning the intermemdiate transformation results to separate variables.\n",
    "results = (spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "           .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "           .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "           .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "           .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "           .where(F.col(\"word\") != \"\")\n",
    "           .groupby(\"word\")\n",
    "           .count()\n",
    "           )\n",
    "results.orderBy(F.col(\"count\").desc()).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6595"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3 return the number of distinct words\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6595\n"
     ]
    }
   ],
   "source": [
    "def getNumDistinctWords(inputPath: str, spark: SparkSession = spark) -> int:\n",
    "    results = (\n",
    "        spark.read.text(inputPath)\n",
    "        .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "        .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "        .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "        .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "        .where(F.col(\"word\") != \"\")\n",
    "        .groupby(\"word\").count()\n",
    "    )\n",
    "    return results.count()\n",
    "\n",
    "print(getNumDistinctWords(\"./data/gutenberg_books/1342-0.txt\", spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'count']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   imitation|    1|\n",
      "|     solaced|    1|\n",
      "|premeditated|    1|\n",
      "|     elevate|    1|\n",
      "|   destitute|    1|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exercise 3.4 list 5 words that only show up once in the input text\n",
    "results.where(F.col(\"count\") == 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[firstchar: string, count: bigint]\n",
      "+---------+-----+\n",
      "|firstchar|count|\n",
      "+---------+-----+\n",
      "|        t|16101|\n",
      "|        a|13684|\n",
      "|        h|10419|\n",
      "|        w| 9091|\n",
      "|        s| 8791|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exercise 3.5a list most populate first letter\n",
    "def getNumDistinctFirstChar(inputPath: str, spark: SparkSession = spark) -> int:\n",
    "    results = (\n",
    "        spark.read.text(inputPath)\n",
    "        .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "        .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "        .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "        .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "        .where(F.col(\"word\") != \"\")\n",
    "        .select(F.substring(F.col(\"word\"), 1, 1).alias(\"firstchar\"))\n",
    "        .groupby(\"firstchar\").count()\n",
    "        .orderBy(F.desc(F.col(\"count\")))\n",
    "    )\n",
    "    return results\n",
    "\n",
    "res = getNumDistinctFirstChar(\"./data/gutenberg_books/1342-0.txt\", spark)\n",
    "\n",
    "print(res)\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|firstchar|count|\n",
      "+---------+-----+\n",
      "|        a|13684|\n",
      "|        i| 8202|\n",
      "|        o| 7199|\n",
      "|        e| 3377|\n",
      "|        u| 1060|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.where(F.col(\"firstchar\").isin(\"a\", \"e\", \"i\", \"o\", \"u\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|isvowel|sum(count)|\n",
      "+-------+----------+\n",
      "|   true|     33522|\n",
      "|  false|     88653|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.col(\"firstchar\").isin(\"a\", \"e\", \"i\", \"o\", \"u\").alias(\"isvowel\"), F.col(\"count\")).groupby(\"isvowel\").sum().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# the following code fails because count() and sum() generate dataframe objects. DataFrame objects don't have a \"sum\" method\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirstchar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\n",
      "File \u001b[0;32m~/data-analysis-pyspark/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3127\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \n\u001b[1;32m   3096\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "# the following code fails because count() and sum() generate dataframe objects. DataFrame objects don't have a \"sum\" method\n",
    "res.groupby(\"firstchar\").count().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
