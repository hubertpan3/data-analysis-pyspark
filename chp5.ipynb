{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Joining and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/07/26 21:27:54 WARN Utils: Your hostname, LAPTOP-CDHH1LA0 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/07/26 21:27:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/26 21:27:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======>                                                    (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# pull in the original logs dataframe\n",
    "import os \n",
    "DIRECTORY = \"./data/broadcast_logs\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"), # the path to the target file is the only mandatory param\n",
    "    sep=\"|\", # field delimiter \",\" by default but this can have issues if dealing with text that contains commas\n",
    "    header = True, # indicates if the file has a header row, can manually set column names with \"schema\" param\n",
    "    inferSchema=True, # tells python to guess at schema, can manually specify schema as well\n",
    "    timestampFormat = \"yyyy-MM-dd\", # tells python how to parse timestamps\n",
    ").drop(\n",
    "    \"BroadcastLogID\", \"SequenceNO\"\n",
    ").withColumn(\"duration_seconds\", (\n",
    "        F.col(\"Duration\").substr(1,2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4,2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7,2).cast(\"int\")\n",
    "    )\n",
    ")\n",
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n",
      "+---------------+------------+---------+\n",
      "|LogIdentifierID|LogServiceID|PrimaryFG|\n",
      "+---------------+------------+---------+\n",
      "|           13ST|        3157|        1|\n",
      "|         2000SM|        3466|        1|\n",
      "|           70SM|        3883|        1|\n",
      "|           80SM|        3590|        1|\n",
      "|           90SM|        3470|        1|\n",
      "+---------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pull in the log_identifier link table and only keep the rows where PrimaryFG == 1\n",
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").where(F.col(\"PrimaryFG\") == 1)\n",
    "log_identifier.printSchema()\n",
    "log_identifier.show(5)\n",
    "\n",
    "# LogIdentifierID is the channel identifier\n",
    "# LogServiceID is the channel key which serves as the Foreign key used to map to the primary table\n",
    "# PrimaryFG is a boolean flag indicating if the channel is the primary one (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining tables in PySpark\n",
    "\n",
    "Bare bones join recipe:\n",
    "\n",
    "```\n",
    "[LEFT_DF].join(\n",
    "    [RIGHT_DF],\n",
    "    on = [PREDICATES],\n",
    "    how = [METHOD]\n",
    ")\n",
    "```\n",
    "\n",
    "- LEFT_DF and RIGHT_DF are the dataframes (tables) to join\n",
    "- PREDICATES indicate how we determine which LEFT_DF records match with which RIGHT_DF records\n",
    "    - records with multiple matches in the other dataframe/table are often duplicated depending on the Method\n",
    "    - PREDICATES are any expression that evaluate to TRUE or FALSE indicating a match or no-match\n",
    "        - \"equi-joins\" where you are testing for equality btw identically named columns can be specified by just listing the target columns\n",
    "        - compound expressions are valid too: `(logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]) & (logs[\"left_ col\"] < log_identifier[\"right_col\"])`\n",
    "        - multiple and'd together statements can just be listed in a list: ` [left[\"col1\"] == right[\"colA\"], left[\"col2\"] > right[\"colB\"], left[\"col3\"] != right[\"colC\"]]`\n",
    "- METHOD indicates how the join happens, basically the same as SQL INNER, OUTER, LEFT INNER ... joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_and_channels = logs.join(\n",
    "    log_identifier,\n",
    "    on = \"LogServiceID\",\n",
    "    how=\"inner\" # inner is the default method, and thus this line could have been omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Naming conventions in the joining world\n",
    "\n",
    "Normally PySpark won't let you create a data frame with two columns that share the same name.\n",
    "If you attempt to use `withColumn` with an existing dataframe name, you will simply overwrite/shadow the existing one.\n",
    "However this can break down with joining when you don't use the standard \"equi-join\" route.\n",
    "\n",
    "e.g. it will happily join two dataframes with conflicting column names, but when you try to use the columns with conflicting names it is ambiguous which one you want to use leading to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_and_channels_verbose_with_dup = logs.join(\n",
    "    log_identifier, logs[\"LogServiceId\"] == log_identifier[\"LogServiceId\"]\n",
    ")\n",
    "\n",
    "# generates a dataframe with two \"LogServiceId\" columns\n",
    "logs_and_channels_verbose_with_dup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AMBIGUOUS_REFERENCE] Reference `LogServiceId` is ambiguous, could be: [`LogServiceId`, `LogServiceId`].\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    logs_and_channels_verbose_with_dup.select(\"LogServiceId\")\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can remove one of the duplicate columns by specifying the one to drop as origin information is retained\n",
    "logs_and_channels_verbose_with_dup_dropped = logs_and_channels_verbose_with_dup.drop(log_identifier[\"LogServiceId\"])\n",
    "# with the duplicate column dropped, we can successfully select the LogServiceId column\n",
    "logs_and_channels_verbose_with_dup_dropped.select(\"LogServiceId\")\n",
    "logs_and_channels_verbose_with_dup_dropped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively you can avoid all of this by using the simplified syntax for joining\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceId\")\n",
    "logs_and_channels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LogServiceId: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you need to keep the duplicate columns and want to use F.col function, you can include embed src table names as aliases in the joined table.\n",
    "logs_and_channels_with_aliases = logs.alias(\"left\").join(\n",
    "    log_identifier.alias(\"right\"),\n",
    "    logs[\"LogServiceId\"] == log_identifier[\"LogServiceId\"]\n",
    "    )\n",
    "\n",
    "# now you can use the source aliases with F.col to drop the duplicate columns\n",
    "logs_and_channels_with_aliases.drop(F.col(\"right.LogServiceId\")).select(\"LogServiceId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we can join the category and program class information\n",
    "\n",
    "DIRECTORY = \"./data/broadcast_logs\"\n",
    "\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep = \"|\",\n",
    "    header = True,\n",
    "    inferSchema = True,\n",
    ").select(\n",
    "    \"CategoryId\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\")\n",
    ")\n",
    "\n",
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassId\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log = (logs_and_channels\n",
    "            .join(cd_category, \"CategoryId\", how=\"left\")\n",
    "            .join(cd_program_class, \"ProgramClassID\", how = \"left\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Excercises\n",
    "\n",
    "###### 5.1\n",
    "Assume two tables, left and right, each containing a column named my_column. What is the result of this code?\n",
    "```python\n",
    "one = left.join(right, how=\"left_semi\", on=\"my_column\") # acts as a filter\n",
    "two = left.join(right, how=\"left_anti\", on=\"my_column\")\n",
    " \n",
    "one.union(two) # regenerates left?\n",
    "```\n",
    "\n",
    "###### 5.2\n",
    "Assume two data frames, red and blue. Which is the appropriate join to use in red.join(blue, ...) if you want to join red and blue and keep all the records satisfying the predicate?\n",
    "Inner\n",
    "\n",
    "###### 5.3\n",
    "Assume two data frames, red and blue. Which is the appropriate join to use in red.join(blue, ...) if you want to join red and blue and keep all the records satisfying the predicate and the records in the blue table?\n",
    "Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Summarizing the data via groupby and GroupedData\n",
    "\n",
    "Basically the summary stats from earlier but on data that has been collapsed into a smaller set of numbers with group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======>                                                   (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------+--------------+\n",
      "|ProgramClassCD|ProgramClass_Description              |duration_total|\n",
      "+--------------+--------------------------------------+--------------+\n",
      "|PGR           |PROGRAM                               |20992510      |\n",
      "|COM           |COMMERCIAL MESSAGE                    |3519163       |\n",
      "|PFS           |PROGRAM FIRST SEGMENT                 |1344762       |\n",
      "|SEG           |SEGMENT OF A PROGRAM                  |1205998       |\n",
      "|PRC           |PROMOTION OF UPCOMING CANADIAN PROGRAM|880600        |\n",
      "|PGI           |PROGRAM INFOMERCIAL                   |679182        |\n",
      "|PRO           |PROMOTION OF NON-CANADIAN PROGRAM     |335701        |\n",
      "|OFF           |SCHEDULED OFF AIR TIME PERIOD         |142279        |\n",
      "|ID            |NETWORK IDENTIFICATION MESSAGE        |74926         |\n",
      "|NRN           |No recognized nationality             |59686         |\n",
      "|MAG           |MAGAZINE PROGRAM                      |57622         |\n",
      "|PSA           |PUBLIC SERVICE ANNOUNCEMENT           |51214         |\n",
      "|SO            |MAY IDENTIFY THE SIGN ON\\OFF OF A DAY |32509         |\n",
      "|OFT           |OFF AIR DUE TO TECHNICAL DIFFICULTY   |18263         |\n",
      "|LOC           |LOCAL ADVERTISING                     |13294         |\n",
      "|MVC           |MUSIC VIDEO CLIP                      |7907          |\n",
      "|REG           |REGIONAL                              |6749          |\n",
      "|MER           |MERCHANDISING                         |1680          |\n",
      "|SPO           |SPONSORSHIP MESSAGE                   |1544          |\n",
      "|SOL           |SOLICITATION MESSAGE                  |596           |\n",
      "|MOS           |Mosaic                                |NULL          |\n",
      "|COR           |CORNERSTONE                           |NULL          |\n",
      "+--------------+--------------------------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    full_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\")# creating a \"transitional\" GroupedData object that can't be shown/inspected. \n",
    "    # Basically a dict where the groupby columns are keys, and the rest of the columns (and their grouped rows) are in a \"cell\" awaiting a aggregation function that will promote\n",
    "    # them to a bonafide column again.\n",
    "    .agg( # section where we apply \"aggregation\" function(s) to promote columns from the holding cell\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\")# promotes the \"duration_seconds\" column to a summary column called \"duration_total\" \n",
    "        ) \n",
    "    .orderBy(\"duration_total\", ascending=False) # applying sort\n",
    "    .show(100, False) # inspecting the first 100 rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------+--------------+\n",
      "|ProgramClassCD|ProgramClass_Description              |duration_total|\n",
      "+--------------+--------------------------------------+--------------+\n",
      "|PGR           |PROGRAM                               |20992510      |\n",
      "|COM           |COMMERCIAL MESSAGE                    |3519163       |\n",
      "|PFS           |PROGRAM FIRST SEGMENT                 |1344762       |\n",
      "|SEG           |SEGMENT OF A PROGRAM                  |1205998       |\n",
      "|PRC           |PROMOTION OF UPCOMING CANADIAN PROGRAM|880600        |\n",
      "|PGI           |PROGRAM INFOMERCIAL                   |679182        |\n",
      "|PRO           |PROMOTION OF NON-CANADIAN PROGRAM     |335701        |\n",
      "|OFF           |SCHEDULED OFF AIR TIME PERIOD         |142279        |\n",
      "|ID            |NETWORK IDENTIFICATION MESSAGE        |74926         |\n",
      "|NRN           |No recognized nationality             |59686         |\n",
      "|MAG           |MAGAZINE PROGRAM                      |57622         |\n",
      "|PSA           |PUBLIC SERVICE ANNOUNCEMENT           |51214         |\n",
      "|SO            |MAY IDENTIFY THE SIGN ON\\OFF OF A DAY |32509         |\n",
      "|OFT           |OFF AIR DUE TO TECHNICAL DIFFICULTY   |18263         |\n",
      "|LOC           |LOCAL ADVERTISING                     |13294         |\n",
      "|MVC           |MUSIC VIDEO CLIP                      |7907          |\n",
      "|REG           |REGIONAL                              |6749          |\n",
      "|MER           |MERCHANDISING                         |1680          |\n",
      "|SPO           |SPONSORSHIP MESSAGE                   |1544          |\n",
      "|SOL           |SOLICITATION MESSAGE                  |596           |\n",
      "|MOS           |Mosaic                                |NULL          |\n",
      "|COR           |CORNERSTONE                           |NULL          |\n",
      "+--------------+--------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively the \"aggs\" method can acept a dictionary of columnName: String -> aggregationFunctionName: String\n",
    "# so the prior code can be rewritten as:\n",
    "\n",
    "(\n",
    "    full_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\")# creating a \"transitional\" GroupedData object that can't be shown/inspected. \n",
    "    # Basically a dict where the groupby columns are keys, and the rest of the columns (and their grouped rows) are in a \"cell\" awaiting a aggregation function that will promote\n",
    "    # them to a bonafide column again.\n",
    "    .agg( # section where we apply \"aggregation\" function(s) to promote columns from the holding cell\n",
    "        {\"duration_seconds\": \"sum\"}\n",
    "    )\n",
    "    .withColumnRenamed(\"sum(duration_seconds)\", \"duration_total\")\n",
    "    .orderBy(\"duration_total\", ascending=False) # applying sort\n",
    "    .show(100, False) # inspecting the first 100 rows\n",
    ")\n",
    "\n",
    "# this style allows for more flexible prototyping as you can use \"*\" to refer to all columns. Author is not a fan b/c column renaming has to happen in a separate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregations with custom column definitions (virtual columns)\n",
    "\n",
    "commercial_time_virtual_column = ( # define a virtual column such that\n",
    "    F.when(\n",
    "        F.trim(F.col(\"ProgramClassCD\")).isin( # when the trimmed ProgramClassCD value is in the following set\n",
    "            [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"PSA\", \"MAG\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "        ),\n",
    "        F.col(\"duration_seconds\"), # return the row associated \"duration_seconds\" value\n",
    "    ).otherwise(0) # return zero\n",
    ")\n",
    "\n",
    "# note that multiple \"when\"'s can be chained together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------+---------------------+\n",
      "|LogIdentifierID|duration_commericial|duration_total|commercial_ratio     |\n",
      "+---------------+--------------------+--------------+---------------------+\n",
      "|CIMT           |775                 |775           |1.0                  |\n",
      "|MSET           |2700                |2700          |1.0                  |\n",
      "|TLNSP          |15480               |15480         |1.0                  |\n",
      "|TELENO         |17790               |17790         |1.0                  |\n",
      "|HPITV          |13                  |13            |1.0                  |\n",
      "|TANG           |8125                |8125          |1.0                  |\n",
      "|MMAX           |23333               |23582         |0.9894410991434145   |\n",
      "|MPLU           |20587               |20912         |0.9844586840091814   |\n",
      "|INVST          |20094               |20470         |0.9816316560820714   |\n",
      "|ZT�L�          |21542               |21965         |0.9807420896881403   |\n",
      "|RAPT           |17916               |18279         |0.9801411455768915   |\n",
      "|CANALD         |21437               |21875         |0.9799771428571429   |\n",
      "|ONEBMS         |18084               |18522         |0.9763524457402009   |\n",
      "|CANALVIE       |20780               |21309         |0.975174808766249    |\n",
      "|unis           |11630               |11998         |0.9693282213702283   |\n",
      "|CIVM           |11370               |11802         |0.9633960345704118   |\n",
      "|TV5            |10759               |11220         |0.9589126559714795   |\n",
      "|LEAF           |11526               |12034         |0.9577862722286854   |\n",
      "|VISION         |12946               |13621         |0.950444167094927    |\n",
      "|CJIL           |3904                |4213          |0.9266555898409684   |\n",
      "|CFRS           |15988               |17789         |0.8987576592276125   |\n",
      "|CFAP           |15880               |17681         |0.8981392455177875   |\n",
      "|RURAL          |2220                |2550          |0.8705882352941177   |\n",
      "|CHNU           |4224                |4921          |0.8583621215200162   |\n",
      "|CIIT           |2942                |3596          |0.818131256952169    |\n",
      "|CFKM           |15976               |19582         |0.8158512920028598   |\n",
      "|CFKS           |15975               |19581         |0.8158418875440479   |\n",
      "|V              |15949               |19557         |0.8155136268343816   |\n",
      "|CFJP           |15925               |19533         |0.8152869502892541   |\n",
      "|CFTU           |3975                |7619          |0.5217220107625673   |\n",
      "|CHEK           |46317               |100372        |0.4614533933766389   |\n",
      "|VRAK           |15900               |38050         |0.4178712220762155   |\n",
      "|CKVR           |40641               |105861        |0.3839090883328138   |\n",
      "|CHAU           |1080                |2880          |0.375                |\n",
      "|CHWI           |36996               |105816        |0.3496257654797006   |\n",
      "|CFPL           |36796               |105616        |0.348394182699591    |\n",
      "|CIVI           |36570               |105330        |0.34719453147251494  |\n",
      "|CESS           |35590               |105700        |0.33670766319772943  |\n",
      "|CFVS           |30660               |91130         |0.33644244485899266  |\n",
      "|CFGS           |30735               |91500         |0.3359016393442623   |\n",
      "|CHRO           |35000               |107345        |0.32605151613955     |\n",
      "|ASN            |34590               |106920        |0.3235129068462402   |\n",
      "|CHAT           |29595               |92665         |0.31937624777424056  |\n",
      "|CIHF           |32610               |108224        |0.30131948551153165  |\n",
      "|CHNB2          |32597               |108251        |0.30112423903705277  |\n",
      "|CFRE           |32471               |108109        |0.30035427207725535  |\n",
      "|CKMI           |32317               |107956        |0.2993534402904887   |\n",
      "|CIII           |32203               |107812        |0.298695878009869    |\n",
      "|CFSK           |32276               |108069        |0.29866104063144844  |\n",
      "|CKND           |31056               |107971        |0.2876327902862806   |\n",
      "|CFTM           |30162               |105732        |0.2852684144818976   |\n",
      "|CJCB           |30440               |107990        |0.28187795166219093  |\n",
      "|CFTO           |30423               |108003        |0.2816866198161162   |\n",
      "|CKLT           |30410               |107990        |0.28160014816186685  |\n",
      "|CKCW           |30410               |107990        |0.28160014816186685  |\n",
      "|CJCH           |30365               |107930        |0.2813397572500695   |\n",
      "|CJOH           |30408               |108093        |0.2813133135355666   |\n",
      "|CHBX           |30352               |108007        |0.28101882285407426  |\n",
      "|CFCF           |30298               |107878        |0.28085429837408926  |\n",
      "|CKNY           |30327               |107982        |0.28085236428293603  |\n",
      "|CITO           |30317               |107972        |0.2807857592709221   |\n",
      "|CICI           |30321               |108036        |0.2806564478507164   |\n",
      "|CKY            |30089               |107669        |0.27945833991213814  |\n",
      "|CKCO           |30153               |107928        |0.2793806982432733   |\n",
      "|CKCK           |30125               |107856        |0.27930759531226823  |\n",
      "|CFQC           |30075               |107986        |0.2785083251532606   |\n",
      "|CFCM           |29364               |105894        |0.2772961640886169   |\n",
      "|CIPA           |29756               |107742        |0.27617827773755826  |\n",
      "|CHLT           |29156               |105686        |0.27587381488560453  |\n",
      "|CJPM           |29125               |105655        |0.27566135062230845  |\n",
      "|CHEM           |29096               |105626        |0.2754624808285839   |\n",
      "|CFER           |29026               |105556        |0.27498200007578916  |\n",
      "|CICC           |29305               |107731        |0.2720201241982345   |\n",
      "|CFTK           |24900               |91785         |0.27128615786893284  |\n",
      "|CIVT           |28851               |108232        |0.26656626506024095  |\n",
      "|CHBC           |28785               |108000        |0.26652777777777775  |\n",
      "|CHAN           |28618               |107938        |0.2651336878578443   |\n",
      "|CJDC           |23985               |91470         |0.2622171203673336   |\n",
      "|CITY           |29121               |112061        |0.25986739365167183  |\n",
      "|CISA           |27296               |108521        |0.2515273541526525   |\n",
      "|CFCNL          |27239               |108434        |0.2512034970581183   |\n",
      "|CFCN           |27199               |108424        |0.2508577436729875   |\n",
      "|CFRN6          |27275               |108740        |0.25082766231377596  |\n",
      "|CITV           |27142               |108442        |0.25029047785913205  |\n",
      "|CICT           |27084               |108398        |0.24985700843188988  |\n",
      "|CFMT           |25645               |102688        |0.24973706762231224  |\n",
      "|CFRN           |27034               |108409        |0.24937043972363918  |\n",
      "|ALLTV          |5736                |23126         |0.2480325175127562   |\n",
      "|GAME           |26155               |107889        |0.24242508504110707  |\n",
      "|OUTDR          |26477               |109607        |0.2415630388570073   |\n",
      "|CHEX           |23154               |97329         |0.2378941528218722   |\n",
      "|CHMI           |26777               |113305        |0.23632672874100877  |\n",
      "|CHEX02         |22592               |95654         |0.2361845819307086   |\n",
      "|CKEM           |26760               |113308        |0.23617043809792776  |\n",
      "|CKVU           |26692               |113367        |0.2354477052405021   |\n",
      "|CKAL           |26652               |113368        |0.23509279514501447  |\n",
      "|DIY            |26147               |112587        |0.23223818025171644  |\n",
      "|CJNT           |26220               |112904        |0.23223269326153193  |\n",
      "|WTN            |24576               |107500        |0.2286139534883721   |\n",
      "|CHCH           |24392               |106997        |0.22796900847687318  |\n",
      "|COOK           |25161               |111561        |0.2255358055234356   |\n",
      "|FASH           |24800               |111320        |0.22278117139777218  |\n",
      "|LIFENT         |24211               |108901        |0.22232119080632867  |\n",
      "|WFN            |24600               |111498        |0.2206317602109455   |\n",
      "|CKWS           |21237               |96289         |0.22055478818972052  |\n",
      "|CKWS1          |21236               |96287         |0.22054898376727908  |\n",
      "|FOOD           |24479               |111088        |0.22035683422151808  |\n",
      "|FYI            |24369               |110769        |0.21999837499661457  |\n",
      "|BBCCND         |24173               |110578        |0.21860587096890882  |\n",
      "|DTOUR          |24059               |110971        |0.21680439033621396  |\n",
      "|ZEST           |23070               |107945        |0.21371994997452406  |\n",
      "|DIVA           |23453               |109961        |0.21328471003355734  |\n",
      "|CAVE           |23410               |109990        |0.2128375306846077   |\n",
      "|MAKE           |23405               |110196        |0.21239427928418456  |\n",
      "|FXC            |23449               |110414        |0.21237343090550112  |\n",
      "|TRV            |23265               |109713        |0.21205326624921386  |\n",
      "|E!             |23255               |109745        |0.21190031436511914  |\n",
      "|OWN            |23213               |109612        |0.21177425829288765  |\n",
      "|CKSA           |21842               |103387        |0.211264472322439    |\n",
      "|TDC            |23035               |109615        |0.21014459699858595  |\n",
      "|HGTV           |22970               |109420        |0.20992505940413086  |\n",
      "|HISTORY        |22883               |109288        |0.20938254886172317  |\n",
      "|COSMO          |22864               |109263        |0.20925656443626844  |\n",
      "|DEJA           |22865               |109325        |0.2091470386462383   |\n",
      "|ANIMAL         |22860               |109445        |0.20887203618255745  |\n",
      "|CEVASI         |22335               |107180        |0.20838775891024444  |\n",
      "|SPARK          |22725               |109425        |0.20767649074708705  |\n",
      "|CASA           |22622               |109022        |0.2074994037900607   |\n",
      "|FXX            |22640               |109220        |0.2072880424830617   |\n",
      "|13ST           |22567               |108982        |0.20707089244095356  |\n",
      "|CMT            |22542               |108942        |0.20691744230875145  |\n",
      "|CHISTO         |22475               |109109        |0.20598667387658212  |\n",
      "|BRAVO          |22370               |108920        |0.20538009548292324  |\n",
      "|TVADIK         |22261               |108661        |0.2048665114438483   |\n",
      "|SERIEP         |22230               |108896        |0.20413972965030855  |\n",
      "|NGWILD         |22160               |108667        |0.2039257548289729   |\n",
      "|TCN            |22140               |108600        |0.20386740331491712  |\n",
      "|DISSCI         |22150               |108730        |0.20371562586222752  |\n",
      "|INVDIS         |22140               |108730        |0.2036236549250437   |\n",
      "|ARTV           |22059               |108519        |0.2032731595388826   |\n",
      "|DISVEL         |22040               |108645        |0.2028625339408164   |\n",
      "|CFEM           |19175               |94580         |0.20273842249947135  |\n",
      "|SHOW           |22027               |108662        |0.2027111593749425   |\n",
      "|COTT           |21960               |108568        |0.2022695453540638   |\n",
      "|RAZER          |21840               |108330        |0.20160620326779286  |\n",
      "|COMGLD         |21645               |108135        |0.20016645859342488  |\n",
      "|EXP            |21521               |107951        |0.19935896842085762  |\n",
      "|GUS            |21445               |107935        |0.1986843933849076   |\n",
      "|MOICIE         |21315               |107715        |0.19788330316111963  |\n",
      "|CHOT           |18512               |94017         |0.19690056053692417  |\n",
      "|SPACE          |21175               |107695        |0.19662008449788757  |\n",
      "|MTV1           |21065               |107585        |0.19579867081842264  |\n",
      "|MUCHM          |20930               |107510        |0.1946795646916566   |\n",
      "|ATN3           |20745               |107267        |0.19339591859565383  |\n",
      "|NGCE           |20390               |107031        |0.19050555446552866  |\n",
      "|ABORIW         |19701               |105846        |0.18612890425712827  |\n",
      "|ABORIN         |19540               |105300        |0.1855650522317189   |\n",
      "|NEWSW          |19592               |106082        |0.18468731735827001  |\n",
      "|RDI            |19500               |105900        |0.18413597733711048  |\n",
      "|ABORI          |19445               |105784        |0.18381796869091735  |\n",
      "|ABORIH         |19405               |106223        |0.18268171676567221  |\n",
      "|BOOK           |19305               |105885        |0.18232044198895028  |\n",
      "|PRIS           |19145               |105545        |0.1813918233928656   |\n",
      "|CFJC           |17140               |95170         |0.180098770620994    |\n",
      "|BBCE           |18665               |103685        |0.18001639581424506  |\n",
      "|FIGHT          |18745               |104229        |0.1798443811223364   |\n",
      "|CTVNC          |18910               |105310        |0.17956509353337766  |\n",
      "|SN360          |18926               |105447        |0.17948353201134218  |\n",
      "|TSN5           |18506               |104996        |0.17625433349841899  |\n",
      "|TRESR          |18385               |105394        |0.17444067024688312  |\n",
      "|TSN3           |18240               |104730        |0.17416213119450014  |\n",
      "|TSN            |17930               |104390        |0.17175974710221287  |\n",
      "|YTV            |17939               |104672        |0.17138298685417303  |\n",
      "|SMITH          |17790               |103894        |0.1712322174524034   |\n",
      "|WILD           |18388               |107721        |0.17070023486599642  |\n",
      "|SPTVA2         |17732               |104132        |0.17028387047209312  |\n",
      "|SPRTVA         |17717               |104117        |0.17016433435462028  |\n",
      "|CKPG           |17020               |100070        |0.17008094333966223  |\n",
      "|BNN            |17702               |104162        |0.16994681361725006  |\n",
      "|SATV           |17772               |104877        |0.16945564804485255  |\n",
      "|TOON           |17691               |104466        |0.16934696456263282  |\n",
      "|DSNYXD         |17722               |104682        |0.16929367035402457  |\n",
      "|TSN1           |17569               |104059        |0.1688369098300003   |\n",
      "|CARTN          |17636               |104776        |0.1683209895395892   |\n",
      "|CP24           |17395               |103860        |0.1674850760639322   |\n",
      "|LCN            |17380               |103780        |0.16746964733089229  |\n",
      "|CBAT           |17349               |103809        |0.1671242377828512   |\n",
      "|CBNT           |17349               |103809        |0.1671242377828512   |\n",
      "|CBHT           |17319               |103779        |0.16688347353510827  |\n",
      "|CBCT           |17319               |103779        |0.16688347353510827  |\n",
      "|CBUT           |17290               |103750        |0.16665060240963855  |\n",
      "|CBRT           |17290               |103750        |0.16665060240963855  |\n",
      "|CBXT           |17290               |103810        |0.1665542818610924   |\n",
      "|CBOT           |17272               |103732        |0.16650599622103113  |\n",
      "|CBLT           |17205               |103665        |0.16596729850962233  |\n",
      "|CBET           |17205               |103665        |0.16596729850962233  |\n",
      "|CBMT           |17145               |103605        |0.1654842912986825   |\n",
      "|BLC            |17280               |104721        |0.16500988340447476  |\n",
      "|CBKT           |16950               |103410        |0.16391064693936758  |\n",
      "|TLTOON         |16890               |103498        |0.1631915592571837   |\n",
      "|CSCN           |16798               |103091        |0.16294341892114733  |\n",
      "|CBWT           |16647               |103107        |0.16145363554365852  |\n",
      "|RDS            |16750               |103855        |0.16128255741177602  |\n",
      "|CFYK           |16645               |103285        |0.16115602459214795  |\n",
      "|SNWST          |16783               |104278        |0.16094478221676672  |\n",
      "|DSNYEN         |16479               |103184        |0.15970499302217397  |\n",
      "|TSN2           |16407               |102987        |0.15931136939613738  |\n",
      "|SNEST          |16573               |104235        |0.1589964982971171   |\n",
      "|RDSINF         |16380               |103200        |0.15872093023255815  |\n",
      "|SNPAC          |16538               |104258        |0.15862571697135952  |\n",
      "|SNONT          |16527               |104235        |0.1585551877968053   |\n",
      "|CJEO           |16728               |105746        |0.15819038072362074  |\n",
      "|CJCO           |16672               |106174        |0.15702526042157214  |\n",
      "|OMAB           |16672               |106174        |0.15702526042157214  |\n",
      "|RDS2           |16180               |103240        |0.1567222006974041   |\n",
      "|FNTSY          |16048               |102545        |0.15649714759373934  |\n",
      "|CJON           |16240               |104555        |0.15532494859165033  |\n",
      "|CJMT           |16408               |105811        |0.15506894368260388  |\n",
      "|ATN1           |15859               |102442        |0.1548095507701919   |\n",
      "|ASIDE          |15780               |102166        |0.1544545152007517   |\n",
      "|PRIDE          |15396               |99859         |0.15417739012006929  |\n",
      "|OMON           |16079               |105457        |0.15246972699773367  |\n",
      "|DOCS           |15610               |102460        |0.15235213741948078  |\n",
      "|ESPN           |15445               |101965        |0.15147354484381895  |\n",
      "|CHNM           |16016               |105845        |0.15131560300439323  |\n",
      "|OMBC           |16016               |105845        |0.15131560300439323  |\n",
      "|CBVT           |14948               |101348        |0.14749181039586376  |\n",
      "|CBOFT          |14798               |101198        |0.14622818632779305  |\n",
      "|FAMCHA         |12600               |86400         |0.14583333333333334  |\n",
      "|ATN5           |14676               |101181        |0.1450469949891778   |\n",
      "|CBFT           |14648               |101048        |0.14496081070382394  |\n",
      "|CHFD           |14215               |98125         |0.14486624203821655  |\n",
      "|NICK           |14290               |100935        |0.1415762619507604   |\n",
      "|CBLFT          |14228               |100658        |0.1413499175425699   |\n",
      "|CKTM           |14018               |100418        |0.13959648668565397  |\n",
      "|CKSH           |14018               |100418        |0.13959648668565397  |\n",
      "|CBKFT          |14018               |100448        |0.13955479452054795  |\n",
      "|CBUFT          |14018               |100448        |0.13955479452054795  |\n",
      "|CBXFT          |14018               |100448        |0.13955479452054795  |\n",
      "|CBWFT          |14018               |100448        |0.13955479452054795  |\n",
      "|CJBRT          |13958               |100418        |0.13899898424585233  |\n",
      "|CKTV           |13838               |100418        |0.13780397936624908  |\n",
      "|CBAFT          |13839               |100479        |0.13773027199713372  |\n",
      "|BC1            |13898               |100958        |0.13766120564987422  |\n",
      "|CKPR           |13050               |94965         |0.13741904912336123  |\n",
      "|CHLF           |1960                |14439         |0.13574347253964955  |\n",
      "|ATN9           |13312               |99792         |0.13339746673080005  |\n",
      "|CHMG           |12748               |96870         |0.1315990502735625   |\n",
      "|ATN2           |12675               |99335         |0.12759853022600293  |\n",
      "|CFHD           |13103               |103223        |0.12693876364763668  |\n",
      "|OMQC           |13103               |103223        |0.12693876364763668  |\n",
      "|ATN10          |12397               |98866         |0.12539194465235773  |\n",
      "|AMIFR          |11957               |96120         |0.12439658759883479  |\n",
      "|CBC            |12275               |98735         |0.12432268192636856  |\n",
      "|CITS           |12120               |98727         |0.12276277006290072  |\n",
      "|SCSD04         |10600               |86400         |0.12268518518518519  |\n",
      "|DSNYFR         |11595               |98312         |0.11794084140288062  |\n",
      "|CITL           |11140               |95687         |0.11642124844545236  |\n",
      "|FACHV2         |11100               |97848         |0.11344125582536178  |\n",
      "|ATN6           |10721               |97121         |0.110388072610455    |\n",
      "|CKCS           |10657               |97158         |0.10968731344819778  |\n",
      "|CKES           |10572               |97019         |0.10896834640637401  |\n",
      "|FTV            |10129               |94841         |0.10679980177349459  |\n",
      "|FACHTV         |10129               |94841         |0.10679980177349459  |\n",
      "|SCSD01         |8872                |86400         |0.10268518518518518  |\n",
      "|AMITV          |9581                |98293         |0.09747387911651897  |\n",
      "|SRC            |9248                |95648         |0.09668785547005687  |\n",
      "|FPTV           |8799                |95285         |0.09234402056986935  |\n",
      "|TTV            |8278                |92687         |0.08931133815961245  |\n",
      "|ATN7           |8451                |94995         |0.08896257697773567  |\n",
      "|CKNO           |7926                |96123         |0.08245685215817235  |\n",
      "|CHRGD          |7808                |95431         |0.08181827707977492  |\n",
      "|MVOLA          |6864                |91851         |0.07472972531600092  |\n",
      "|SALT           |6852                |93541         |0.07325130156829626  |\n",
      "|YOOP           |6584                |92984         |0.07080788092575066  |\n",
      "|NATUR          |6125                |92847         |0.06596874427822116  |\n",
      "|SN2            |6095                |92495         |0.06589545380831396  |\n",
      "|FAMJR          |5438                |86400         |0.06293981481481481  |\n",
      "|THTV           |5478                |92253         |0.059380182758284286 |\n",
      "|DSNYJR         |4917                |91602         |0.0536778672954739   |\n",
      "|SE04           |4431                |86400         |0.051284722222222225 |\n",
      "|TCC            |4281                |91081         |0.04700211899298427  |\n",
      "|NEOCRI         |4246                |90646         |0.0468415594731152   |\n",
      "|MAGINO         |4018                |91221         |0.04404687517128731  |\n",
      "|SCSD03         |3740                |86400         |0.043287037037037034 |\n",
      "|BBCKID         |3689                |92104         |0.040052549292104576 |\n",
      "|SE03           |3453                |86400         |0.03996527777777778  |\n",
      "|TMN2           |3393                |86400         |0.03927083333333333  |\n",
      "|TMN4           |3386                |86400         |0.039189814814814816 |\n",
      "|SCSD02         |3279                |86400         |0.03795138888888889  |\n",
      "|CICA           |3383                |90140         |0.037530508098513424 |\n",
      "|ZEETV          |3345                |89971         |0.037178646452745885 |\n",
      "|SLVSC          |3228                |89722         |0.03597779808742561  |\n",
      "|TMN3           |3062                |86400         |0.03543981481481481  |\n",
      "|MPIX2          |2951                |86400         |0.03415509259259259  |\n",
      "|MOVIEP         |2798                |86400         |0.03238425925925926  |\n",
      "|70SM           |2869                |89326         |0.03211830821933144  |\n",
      "|SE02           |2477                |86400         |0.028668981481481483 |\n",
      "|2000SM         |2556                |89311         |0.02861909507227553  |\n",
      "|TMN1           |2393                |86400         |0.027696759259259258 |\n",
      "|SE01           |2278                |86400         |0.02636574074074074  |\n",
      "|CPOP           |1777                |86400         |0.02056712962962963  |\n",
      "|80SM           |1803                |88352         |0.020407008330315102 |\n",
      "|90SM           |1702                |88372         |0.01925949395736206  |\n",
      "|ATN8           |1623                |88153         |0.01841117148593922  |\n",
      "|SAB            |616                 |87207         |0.00706365314710975  |\n",
      "|IDNR           |297                 |86530         |0.0034323356061481567|\n",
      "|OTN1           |240                 |86730         |0.002767208578346593 |\n",
      "|CFTF           |0                   |1805          |0.0                  |\n",
      "|CFTV           |0                   |102           |0.0                  |\n",
      "|CKRT           |0                   |14400         |0.0                  |\n",
      "|OTN3           |0                   |86400         |0.0                  |\n",
      "|PLAY           |0                   |86400         |0.0                  |\n",
      "|SKIN           |0                   |86400         |0.0                  |\n",
      "|SNONE          |0                   |1800          |0.0                  |\n",
      "|PENT           |0                   |86400         |0.0                  |\n",
      "|ATN13          |0                   |86400         |0.0                  |\n",
      "|TIMESN         |0                   |86400         |0.0                  |\n",
      "|ATN11          |0                   |86400         |0.0                  |\n",
      "|ATN14          |0                   |86400         |0.0                  |\n",
      "|TENCH2         |0                   |86400         |0.0                  |\n",
      "|ZOOM           |0                   |86400         |0.0                  |\n",
      "|EURO           |0                   |NULL          |NULL                 |\n",
      "|NINOS          |0                   |NULL          |NULL                 |\n",
      "+---------------+--------------------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next we can use the virtual column to determine the commercial ration\n",
    "\n",
    "answer = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(commercial_time_virtual_column).alias(\"duration_commericial\"),\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    ).withColumn(\n",
    "        \"commercial_ratio\", F.col(\"duration_commericial\")/F.col(\"duration_total\")\n",
    "    )\n",
    ")\n",
    "\n",
    "answer.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Dealing with null values.\n",
    "\n",
    "`null` values indicate missing data. Typically we can either drop the associated record, or fill (replace) the null with some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+--------+\n",
      "|  id|    name|price|quantity|\n",
      "+----+--------+-----+--------+\n",
      "|   2|product2|  3.0|       5|\n",
      "|   3|product3| 8.99|       8|\n",
      "|NULL|    NULL| NULL|    NULL|\n",
      "|   4|    NULL| NULL|    NULL|\n",
      "|   5|product5| NULL|    NULL|\n",
      "|   6|product6| 2.99|    NULL|\n",
      "+----+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generating test data frame with null values\n",
    "\n",
    "has_nulls = spark.createDataFrame(\n",
    "    [\n",
    "        [2, \"product2\", 3.00, 5],\n",
    "        [3, \"product3\", 8.99, 8],\n",
    "        [None, None, None, None],\n",
    "        [4, None, None, None],\n",
    "        [5, \"product5\", None, None],\n",
    "        [6, \"product6\", 2.99, None],\n",
    "\n",
    "    ],\n",
    "    [\"id\", \"name\", \"price\", \"quantity\"]\n",
    ")\n",
    "\n",
    "has_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------+\n",
      "| id|    name|price|quantity|\n",
      "+---+--------+-----+--------+\n",
      "|  2|product2|  3.0|       5|\n",
      "|  3|product3| 8.99|       8|\n",
      "+---+--------+-----+--------+\n",
      "\n",
      "+---+--------+-----+--------+\n",
      "| id|    name|price|quantity|\n",
      "+---+--------+-----+--------+\n",
      "|  2|product2|  3.0|       5|\n",
      "|  3|product3| 8.99|       8|\n",
      "|  5|product5| NULL|    NULL|\n",
      "|  6|product6| 2.99|    NULL|\n",
      "+---+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping with dropna()\n",
    "# drop na takes three optional arguments\n",
    "# - how -> {\"any\", \"all\"}: Defaults to \"any\". If \"any\" a record is dropped if it contains any null properties. If \"all\" is selected, all properties must be null for the record to be dropped\n",
    "# - thresh \"threshold\" -> {null, int}: Ignored if null, else overrides the \"how\" param so that only records with less than \"thresh\" null properties are dropped\n",
    "# - subset -> {list<cols>}: optional list of columns to focus on when making the drop decision\n",
    "\n",
    "has_nulls.dropna().show()\n",
    "has_nulls.dropna(thresh=2, subset=[\"id\", \"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------+\n",
      "| id|    name|price|quantity|\n",
      "+---+--------+-----+--------+\n",
      "|  2|product2|  3.0|       5|\n",
      "|  3|product3| 8.99|       8|\n",
      "|  0|    NULL|  0.0|       0|\n",
      "|  4|    NULL|  0.0|       0|\n",
      "|  5|product5|  0.0|       0|\n",
      "|  6|product6| 2.99|       0|\n",
      "+---+--------+-----+--------+\n",
      "\n",
      "+----+--------+-----+--------+\n",
      "|  id|    name|price|quantity|\n",
      "+----+--------+-----+--------+\n",
      "|   2|product2|  3.0|       5|\n",
      "|   3|product3| 8.99|       8|\n",
      "|NULL|    NULL|  0.0|       0|\n",
      "|   4|    NULL|  0.0|       0|\n",
      "|   5|product5|  0.0|       0|\n",
      "|   6|product6| 2.99|       0|\n",
      "+----+--------+-----+--------+\n",
      "\n",
      "+----+--------+-----+--------+\n",
      "|  id|    name|price|quantity|\n",
      "+----+--------+-----+--------+\n",
      "|   2|product2|  3.0|       5|\n",
      "|   3|product3| 8.99|       8|\n",
      "|NULL| unknown|  0.0|       0|\n",
      "|   4| unknown|  0.0|       0|\n",
      "|   5|product5|  0.0|       0|\n",
      "|   6|product6| 2.99|       0|\n",
      "+----+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling with fillna \n",
    "# fillna takes two optional arguments\n",
    "# - value : the value to fill na's with, will only fill na values with compatible types\n",
    "# - subset -> {list<cols>}: the list of columns to consider for filling\n",
    "\n",
    "has_nulls.fillna(0).show() # will only fill the numeric values it can target\n",
    "has_nulls.fillna(0, subset=['price', 'quantity']).show() # only fill the price and quantity columns\n",
    "\n",
    "# fillna's can be chained\n",
    "has_nulls.fillna(0, subset=['price', 'quantity']).fillna(\"unknown\", subset=[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------+----------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|commercial_ratio|\n",
      "+---------------+-------------------+--------------+----------------+\n",
      "|BRAVO          |0                  |0             |0.0             |\n",
      "|BBCKID         |0                  |0             |0.0             |\n",
      "|BOOK           |0                  |0             |0.0             |\n",
      "|CBKT           |0                  |0             |0.0             |\n",
      "|CBHT           |0                  |0             |0.0             |\n",
      "|CBAFT          |0                  |0             |0.0             |\n",
      "|ATN9           |0                  |0             |0.0             |\n",
      "|MAKE           |0                  |0             |0.0             |\n",
      "|13ST           |0                  |0             |0.0             |\n",
      "|BBCCND         |0                  |0             |0.0             |\n",
      "+---------------+-------------------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all in one (untested)\n",
    "\n",
    "# imports\n",
    "import os \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# setup\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Getting the Canadian TV channels with the highest/lowest proportion of ads\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# pulling data\n",
    "DIRECTORY=\"./data/broadcast_logs\"\n",
    "\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep = \"|\",\n",
    "    header = True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "cd_program_class = spark.read.csv(\n",
    "    \"./data/broadcast_logs/ReferenceTables/CD_ProgramClass.csv\",\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "# Data processing\n",
    " \n",
    "logs = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\n",
    " \n",
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ),\n",
    ")\n",
    " \n",
    "log_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\n",
    " \n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    " \n",
    "full_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\"\n",
    ")\n",
    " \n",
    "answerS = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "                ),\n",
    "                F.col(\"duration_seconds\"),\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"duration_commercial\"),\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    " \n",
    "answerS.orderBy(\"commercial_ratio\", ascending=False).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+\n",
      "|LogServiceID|   LogDate|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|           Duration|            EndTime|LogEntryDate|ProductionNO|        ProgramTitle|          StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|duration_seconds|\n",
      "+------------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+\n",
      "|        3157|2018-08-01|                  4|                  NULL|        13|              3|                3|            NULL|           NULL|                10|            19|                NULL|           2|2024-07-26 02:00:00|2024-07-26 08:00:00|  2018-08-01|      A39082|   Newlywed and Dead|2024-07-26 06:00:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|       94|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|            20|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:13:45|  2018-08-01|        NULL|15-SPECIALTY CHAN...|2024-07-26 06:13:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:00|  2018-08-01|        NULL|3-PROCTER & GAMBL...|2024-07-26 06:13:45|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:15|  2018-08-01|        NULL|12-CREDIT KARMA-B...|2024-07-26 06:14:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:30|  2018-08-01|        NULL|3-L'OREAL CANADA-...|2024-07-26 06:14:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:45|  2018-08-01|        NULL|11-YUM! BRANDS-Ch...|2024-07-26 06:14:30|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:15:16|  2018-08-01|        NULL|2-PIER 1 IMPORTS ...|2024-07-26 06:14:46|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:15:31|  2018-08-01|        NULL|3-HAVAS EDGE-Trav...|2024-07-26 06:15:16|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:15:46|  2018-08-01|        NULL|2-AUTOTRADER-Inte...|2024-07-26 06:15:31|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:16:01|  2018-08-01|        NULL|11-SLEEP COUNTRY ...|2024-07-26 06:15:46|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:16:16|  2018-08-01|        NULL|11-GENERAL MILLS ...|2024-07-26 06:16:01|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:16:46|  2018-08-01|        NULL|11-PROCTER & GAMB...|2024-07-26 06:16:16|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|            20|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:25:56|  2018-08-01|        NULL|15-SPECIALTY CHAN...|2024-07-26 06:25:26|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:26:11|  2018-08-01|        NULL|11-PROCTER & GAMB...|2024-07-26 06:25:56|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:26:26|  2018-08-01|        NULL|11-LABATT BREWERI...|2024-07-26 06:26:11|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:26:41|  2018-08-01|        NULL|2-IKEA CANADA LTD...|2024-07-26 06:26:26|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:26:57|  2018-08-01|        NULL|11-WAL-MART CANAD...|2024-07-26 06:26:42|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:27:12|  2018-08-01|        NULL|2-AUTOTRADER-Inte...|2024-07-26 06:26:57|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:01:00|2024-07-26 06:28:12|  2018-08-01|        NULL|12-COMWAVE TELECO...|2024-07-26 06:27:12|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "|        3157|2018-08-01|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:28:27|  2018-08-01|        NULL|11-L'OREAL CANADA...|2024-07-26 06:28:12|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|\n",
      "+------------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  6|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  6|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.4\n",
    "left = spark.createDataFrame(\n",
    "    [\n",
    "        [1, \"product2\", 3.00, 5],\n",
    "        [2, \"product3\", 8.99, 8],\n",
    "        [3, None, None, None],\n",
    "        [4, None, None, None],\n",
    "        [5, \"product5\", None, None],\n",
    "        [6, \"product6\", 2.99, None],\n",
    "\n",
    "    ],\n",
    "    [\"id\", \"name\", \"price\", \"quantity\"]\n",
    ")\n",
    "\n",
    "right = spark.createDataFrame(\n",
    "    [\n",
    "        [1, \"product2 desc\"],\n",
    "        [5, \"product 4 desc\"],\n",
    "        [8, \"product 8 desc\"],\n",
    "    ],\n",
    "    [\"id\", \"description\"]\n",
    ")\n",
    "\n",
    "left.join(right, how=\"left_anti\", on=\"id\").select(\"id\").distinct().show()\n",
    "# rewrite left anti join without left_anti\n",
    "left.join(right, how=\"left\", \n",
    "          on=left[\"id\"] == right[\"id\"]).where(\n",
    "              right[\"id\"].isNull()\n",
    "              ).select(left[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|LogIdentifierID|    Undertaking_Name|\n",
      "+---------------+--------------------+\n",
      "|          BRAVO|              Bravo!|\n",
      "|           CBET|Canadian Broadcas...|\n",
      "|          CFTV3|Southshore Broadc...|\n",
      "|           CHEX|591987 B.C. Ltd.,...|\n",
      "|           CICT|Corus Television ...|\n",
      "|           CIMT|Télé Inter-Rives ...|\n",
      "|           CKMI|Corus Television ...|\n",
      "|          FIGHT|      Fight Network |\n",
      "|         SCSD02|Super Channel (fo...|\n",
      "|          SMITH|Smithsonian Chann...|\n",
      "|        STJUICE|   Stingray Juicebox|\n",
      "|            TCN| The Comedy Network |\n",
      "|           TMN3|Crave (The Movie ...|\n",
      "|            TRN|HPItv (formerly T...|\n",
      "|         WARNER|Hollywood Suite 7...|\n",
      "|           13ST|Crime + Investiga...|\n",
      "|          ASIDE|A.Side (formerly ...|\n",
      "|            AUX|A.Side (formerly ...|\n",
      "|          CFCNL|Bell Media Inc., ...|\n",
      "|           CKAL|Rogers Media Inc....|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------------------+--------------+----------------+--------------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|commercial_ratio|    Undertaking_Name|\n",
      "+---------------+-------------------+--------------+----------------+--------------------+\n",
      "|          BRAVO|                  0|             0|             0.0|              Bravo!|\n",
      "|           BOOK|                  0|             0|             0.0|Book Television (...|\n",
      "|           CBKT|                  0|             0|             0.0|Canadian Broadcas...|\n",
      "|           CBHT|                  0|             0|             0.0|Canadian Broadcas...|\n",
      "|          CBAFT|                  0|             0|             0.0|Société Radio-Can...|\n",
      "|           MAKE|                  0|             0|             0.0|Makeful TV (forme...|\n",
      "|           13ST|                  0|             0|             0.0|Crime + Investiga...|\n",
      "|         BBCCND|                  0|             0|             0.0|          BBC Canada|\n",
      "|         ANIMAL|                  0|             0|             0.0|       Animal Planet|\n",
      "|            ASN|                  0|             0|             0.0|CTV Two Atlantic ...|\n",
      "|           VRAK|                  0|             0|             0.0|VRAK (formerly Vr...|\n",
      "|           ARTV|                  0|             0|             0.0|            ICI ARTV|\n",
      "|         CANALD|                  0|             0|             0.0|             Canal D|\n",
      "|            CBC|                  0|             0|             0.0|  English TV Service|\n",
      "|           CBAT|                  0|             0|             0.0|Canadian Broadcas...|\n",
      "|         ABORIN|                  0|             0|             0.0|Aboriginal People...|\n",
      "|            BC1|                  0|             0|             0.0|BC News 1 (former...|\n",
      "|           CBLT|                  0|             0|             0.0|Canadian Broadcas...|\n",
      "|       CANALVIE|                  0|             0|             0.0|           Canal Vie|\n",
      "|           CBFT|                  0|             0|             0.0|Société Radio-Can...|\n",
      "+---------------+-------------------+--------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.5\n",
    "\"\"\"\n",
    "Using the data from the data/broadcast_logs/Call_Signs.csv (careful: the delimiter here is the comma, not the pipe!), \n",
    "add the Undertaking_Name to our final table to display a human-readable description of the channel.\n",
    "\"\"\"\n",
    "\n",
    "call_signs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"Call_Signs.csv\"),\n",
    "    sep = \",\",\n",
    "    header = True \n",
    ").drop(\"UndertakingNO\")\n",
    "\n",
    "call_signs.show()\n",
    "\n",
    "answer.join(call_signs, on=\"LogIdentifierID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------+----------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|commercial_ratio|\n",
      "+---------------+-------------------+--------------+----------------+\n",
      "|BRAVO          |0.0                |NULL          |NULL            |\n",
      "|BBCKID         |0.0                |NULL          |NULL            |\n",
      "|BOOK           |0.0                |NULL          |NULL            |\n",
      "|CBKT           |0.0                |NULL          |NULL            |\n",
      "|CBHT           |0.0                |NULL          |NULL            |\n",
      "|CBAFT          |0.0                |NULL          |NULL            |\n",
      "|ATN9           |0.0                |NULL          |NULL            |\n",
      "|MAKE           |0.0                |NULL          |NULL            |\n",
      "|13ST           |0.0                |NULL          |NULL            |\n",
      "|BBCCND         |0.0                |NULL          |NULL            |\n",
      "+---------------+-------------------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+----------+------------+----------+-------------------+----------------------+---------------+-----------------+----------------+---------------+------------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+---------------+---------+----------+--------------------+--------------+------------------------+\n",
      "|ProgramClassID|CategoryID|LogServiceID|   LogDate|AudienceTargetAgeID|AudienceTargetEthnicID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|FilmClassificationID|ExhibitionID|           Duration|            EndTime|LogEntryDate|ProductionNO|        ProgramTitle|          StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|duration_seconds|LogIdentifierID|PrimaryFG|CategoryCD|Category_Description|ProgramClassCD|ProgramClass_Description|\n",
      "+--------------+----------+------------+----------+-------------------+----------------------+---------------+-----------------+----------------+---------------+------------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+---------------+---------+----------+--------------------+--------------+------------------------+\n",
      "|            19|        13|        3157|2018-08-01|                  4|                  NULL|              3|                3|            NULL|           NULL|                10|                NULL|           2|2024-07-26 02:00:00|2024-07-26 08:00:00|  2018-08-01|      A39082|   Newlywed and Dead|2024-07-26 06:00:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|       94|     NULL|            NULL|           13ST|        1|       07C|SPECIALS,MINI-SER...|          PGR |                 PROGRAM|\n",
      "|            20|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:13:45|  2018-08-01|        NULL|15-SPECIALTY CHAN...|2024-07-26 06:13:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          PRC |    PROMOTION OF UPCO...|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:00|  2018-08-01|        NULL|3-PROCTER & GAMBL...|2024-07-26 06:13:45|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:15|  2018-08-01|        NULL|12-CREDIT KARMA-B...|2024-07-26 06:14:00|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:30|  2018-08-01|        NULL|3-L'OREAL CANADA-...|2024-07-26 06:14:15|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:14:45|  2018-08-01|        NULL|11-YUM! BRANDS-Ch...|2024-07-26 06:14:30|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:30|2024-07-26 06:15:16|  2018-08-01|        NULL|2-PIER 1 IMPORTS ...|2024-07-26 06:14:46|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:15:31|  2018-08-01|        NULL|3-HAVAS EDGE-Trav...|2024-07-26 06:15:16|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:15:46|  2018-08-01|        NULL|2-AUTOTRADER-Inte...|2024-07-26 06:15:31|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "|             3|      NULL|        3157|2018-08-01|               NULL|                  NULL|              1|             NULL|            NULL|           NULL|              NULL|                NULL|        NULL|2024-07-26 00:00:15|2024-07-26 06:16:01|  2018-08-01|        NULL|11-SLEEP COUNTRY ...|2024-07-26 06:15:46|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|            NULL|           13ST|        1|      NULL|                NULL|          COM |      COMMERCIAL MESSAGE|\n",
      "+--------------+----------+------------+----------+-------------------+----------------------+---------------+-----------------+----------------+---------------+------------------+--------------------+------------+-------------------+-------------------+------------+------------+--------------------+-------------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+----------------+---------------+---------+----------+--------------------+--------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/26 22:04:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# exercise 5.6\n",
    "answer2 = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                    [\"COM\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "                ),\n",
    "                F.col(\"duration_seconds\"),\n",
    "            ).when(\n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                    [\"PRC\"]\n",
    "                ),\n",
    "                (0.75 * F.col(\"duration_seconds\"))\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"duration_commercial\"),\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    "    )\n",
    "    # .fillna(0)\n",
    ")\n",
    "answer2.orderBy(\"commercial_ratio\", ascending=False).show(10, False)\n",
    "full_log.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.bob import test_fun\n",
    "\n",
    "test_fun(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|roundedPrice|count|\n",
      "+------------+-----+\n",
      "|         3.0|    2|\n",
      "|         9.0|    1|\n",
      "|        NULL|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/29 07:48:58 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 43685417 ms exceeds timeout 120000 ms\n",
      "24/07/29 07:48:58 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "24/07/29 07:48:59 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:48:59 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:44 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/07/29 07:49:44 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:38701\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "rnd = left.select(F.round(left.price).alias(\"roundedPrice\"))\n",
    "rnd.groupby(\"roundedPrice\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
